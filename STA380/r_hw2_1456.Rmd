---
title: "r_hw2"
date: "8/11/2019"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<TABLE>
<TH>Names</TD>
<TR><TD>Aishwarya Pawar</TD></TR>
<TR><TD>Amey Athaley</TD></TR>
<TR><TD>Kachi Ugo</TD></TR>
<TR><TD>Sadhana Koneni</TD></TR>
</TABLE>

# Question 1 - Green Buildings

```{r}
library(tidyverse)
library(mosaic)

buildings = read.csv('C:/Users/Pranay/OneDrive/Documents/STA 380/STA380/data/greenbuildings.csv')

# only empl_gr - the year on year employment growth in the geographic region - has 74 null values among all columns
sapply(buildings,function(x) sum(is.na(x)))

# very low leasing rates - outlier treatment
ggplot(buildings) +  
  geom_point(aes(x=leasing_rate, y = Rent, fill = factor(green_rating)), shape = 21, alpha = 0.5, size = 2) + 
  theme_classic()
```

## Outlier Treatment:
Like the analyst mentioned, we observe here that the there are few buildings with very low occupancy rates. These buildings also have rents which are varying. These 215 buildings have been removed from further analysis as these could potentially distort the analysis 

```{r}
dim(buildings)
buildings = buildings[buildings$leasing_rate > 10,]
dim(buildings)
```

```{r}
## Calculating the premium on the green buildings - $27.6-$25 which is $2.6
buildings_summ = buildings %>%
  group_by(green_rating)  %>%  # group the data points by green_rating
  summarize(rent.mean = median(Rent))
buildings_summ

# rent distribution in green and non-green buildings: Green buildings have slightly higher rents
ggplot(data=buildings) + 
  geom_boxplot(mapping=aes(x=factor(green_rating),y=log(Rent)))

```

## Class

```{r}

###########********** 1. Class **********###########

buildings$class = ifelse(buildings$class_a == 1, "A", ifelse(buildings$class_b == 1,"B","C") )

buildings_class = buildings %>% group_by(class, green_rating) %>% summarize(rent.median = median(Rent), rent.mean= mean(Rent))

ggplot(data = buildings_class) + 
  geom_bar(mapping = aes(x=class, y=rent.mean, fill=factor(green_rating)),
           position="dodge", stat='identity')

table(buildings$class, buildings$green_rating)


```

Our hypothesis is that the rents are higher for Class A buildings.

Findings: 
We observe that the class of the building makes a difference but green vs. non-green buildings doesn't show any significant changes in rent except for Class C. On further investigation, we find that there are only 7 green buildings so the average rent in this case may not be an accurate representation.  

## Age

```{r}
###########********** 2. Age **********###########
buildings_age = buildings %>%
  mutate(agecat = cut(age, c(-1, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200)))
summary(buildings_age)

buildings_age %>%
  group_by(agecat, green_rating)  %>%  # group the data points by age category
  summarize(rent.mean = mean(Rent), rent.median = median(Rent))

table(buildings_age$agecat, buildings_age$green_rating)

ggplot(data = buildings) + 
  geom_histogram(mapping = aes(x=age, y=stat(density), fill=factor(green_rating)), binwidth = 3)

ggplot(buildings_age) +
  geom_boxplot(mapping=aes(x=agecat,y=Rent, fill=factor(green_rating)))
```

Findings:
Most of the green buildings are less than 4 decades old and we see that as the buildings get older, green buildings tend to have higher rents compared to non-green buildings (we didn't consider green buildings older than 80 yrs as there are very few to make meaningful conclusion about them). But in the initial decades, there is no significant difference in rents between green and non-green categories.

## Amenities

```{r}
###########********** 3. Amenities **********###########
buildings %>%
  group_by(amenities)  %>%  # group the data points by amenities
  summarize(rent.mean = mean(Rent), rent.median = median(Rent))

buildings_am = buildings %>%
  group_by(green_rating) %>%
  summarize(am_pct = sum(amenities==1)/n())
buildings_am

ggplot(data = buildings_am) + 
  geom_bar(mapping = aes(x=green_rating, y=am_pct), stat='identity')

```

Our hypothesis here was that buildings with amenities would have higher rents. 

Findings:
We observed that the mean rent is higher for green buildings with amenities. But, there are very few buildings with amenities in our dataset and we have no information on the extra cost for providing these amenities. Hence, amenities has been ruled out as a factor in our analysis.

## Net Contracts

```{r}
###########********** 4. Net contracts **********########### 
buildings %>%
  group_by(net)  %>%  # group the data points by net contract
  summarize(rent.mean = mean(Rent), rent.median = median(Rent))

buildings_net = buildings %>%
  group_by(green_rating) %>%
  summarize(net_pct = sum(net==1)/n())
buildings_net # green_rating = 1 -> 0.056, green_rating = 0 -> 0.032

ggplot(data = buildings_net) + 
  geom_bar(mapping = aes(x=green_rating, y=net_pct), stat='identity')
```

-Rent is lower when it is a net-contract
-The net-contract buildings form very small percentage of the datapoints(<1%) and we do not have information on the utilities bill. -Thus, we omitted this from our analysis.

## Leasing Rate

```{r}
###########********** 5. Leasing Rate **********########### 
# The leasing rate is higher for green buildings
ggplot(data=buildings) + 
  geom_boxplot(mapping=aes(x=factor(green_rating),y=leasing_rate))

leasing_rate_green = mean(buildings[buildings$green_rating==1,]$leasing_rate)
leasing_rate_nongreen = mean(buildings[buildings$green_rating==0,]$leasing_rate)

print(paste("The leasing rate for green buildings is", leasing_rate_green, "and for non-green buildings is", leasing_rate_nongreen))

```

## Conclusion:
The leasing rates are different for green and non-green buildings. Once we account for the leasing rate in the calculation, we see that the extra cost for green buildings can be recovered in 8.6 years(5mn/(premium for green buildings x leasing rate for green buildings x square footage)). Once it is recovered, the green buildings will earn $581,395 more than a non-green building an year. 

# Question 4 - Market Segmentation

```{r }
library(ggplot2)
library(ggcorrplot)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)
library(tidyverse)
library(dplyr)
```

## Reading data 

```{r}
social = read.csv('C:/Users/Pranay/OneDrive/Documents/STA 380/STA380/data/social_marketing.csv', header=TRUE, row.names=1)
head(social)
social_s = scale(social, center=TRUE, scale=TRUE)

```
## EDA
Upon looking at the data set, we observe that there were only a few rows with values in the spam category with over 7000 zeroes, this was significantly higher than other categories and so it was excluded. We also excluded the chatter and uncategorized columns because they do not tell much about user interests. We used a correlation plot to see how much the variables are related to each other and saw that there was high correlation amogst them.
```{r}
cormat <- round(cor(social_s), 2)
ggcorrplot(cormat, hc.order = TRUE )+
  theme(axis.text.x = element_text(angle=90, vjust=0.6))

categories = c(colnames(social))
max_val = apply(social,2,max)
max.val = as.integer(max_val)
totals=colSums(social)
total.s = as.integer(totals)
n_total = cbind(categories,total.s)
zero_count = colSums(social==0)
zero.count = as.integer(zero_count)
n_zero = cbind(categories,zero.count)
tops = data.frame(cbind(categories,max.val))

#number of tweets for each column
ggplot(data = data.frame(n_total)) + 
  geom_bar(mapping = aes(x=reorder(categories, totals), y=totals), stat='identity')+
  coord_flip()

#number of rows with zeroes for each column
ggplot(data = data.frame(n_zero)) + 
  geom_bar(mapping = aes(x=reorder(categories, zero_count), y=zero_count), stat='identity')+
  coord_flip()

#checking for outliers, maximum value in each column
ggplot(data = tops) + 
  geom_bar(mapping = aes(x=reorder(categories, max_val), y=max_val), stat='identity')+
  coord_flip()
```
## PCA
Due to the number of variables and correlation in our dataset, we used PCA to reduce the number of dimensions but still capture most of the variance. We proceeded to create a plot showing the proportion of variance explained by each principal component and decided that components PC1 to PC6 were adequate to use as input for clustering.
```{r}
social2=social %>% select(-one_of(c("chatter","uncategorized", "spam")))
S = social2/rowSums(social2)

pca_s = prcomp(S, center=TRUE, scale= TRUE)
plot(pca_s)
plot(pca_s, type='l')

pca_var <-  pca_s$sdev ^ 2
pve <- pca_var / sum(pca_var)
#plot of pca variance proportion
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = 'b')

comp = data.frame(pca_s$x[,1:6])
```

## K-Means++ Clustering
In order to choose the number of clusters, we used a for loop to run the k-means++ algorithm on values from 1:15. We then plotted the within sum of squares value from each of those (an elbow plot) and decided to go with 6 clusters.

```{r}
wss = (nrow(comp)-1)*sum(apply(comp,2,var))
for (i in 2:15) 
  wss[i] = sum(kmeans(comp,centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")


#kmeans++ using PCA scores and 6 clusters
set.seed(5)
k = kmeanspp(comp, 6, nstart=25, iter.max=1000)

#Plotting pc1 and pc2
qplot(comp[,1], comp[,2], data=comp, xlab='Component 1', ylab='Component 2')
#Plotting pc1 and pc2 showing clusters
qplot(comp[,1], comp[,2], data=comp, col=factor(k$cluster), xlab='Component 1', 
      ylab='Component 2')

loadings = pca_s$rotation[, 1:6]
clust2 = kmeanspp(loadings, 6, nstart=25, iter.max=1000)
which(clust2$cluster == 1)
which(clust2$cluster == 2)
which(clust2$cluster == 3)
which(clust2$cluster == 4)
which(clust2$cluster == 5)
which(clust2$cluster == 6)

```

To figure out which categories were highly related to each cluster, we looked at the PCA loadings to see the top associated variables. We also clustered the PCA loadings to see which categories were grouped together. We then used these variables to create biplots on the original scaled data and colored the points by cluster.

How are the individual PCs loaded on the original variables?
The top variables associated with each component

```{r}
o1 = order(loadings[,1], decreasing=TRUE)
colnames(S)[head(o1,10)]
colnames(S)[tail(o1,5)]

o2 = order(loadings[,2], decreasing=TRUE)
colnames(S)[head(o2,10)]
colnames(S)[tail(o2,5)]

o3 = order(loadings[,3], decreasing=TRUE)
colnames(S)[head(o3,10)]
colnames(S)[tail(o3,5)]

o4 = order(loadings[,4], decreasing=TRUE)
colnames(S)[head(o4,10)]
colnames(S)[tail(o4,5)]

o5 = order(loadings[,5], decreasing=TRUE)
colnames(S)[head(o5,10)]
colnames(S)[tail(o5,5)]

o6 = order(loadings[,6], decreasing=TRUE)
colnames(S)[head(o6,10)]
colnames(S)[tail(o6,5)]
```
## Market Segments According to Correlated Interests
K-means++ clustering with 6 clusters of sizes 2254, 976, 719, 1208, 1280, 1445:
*	Young to middle-aged women (Cluster 1): Photo sharing, shopping, home & garden are interests largely related with young women in their mid-twenties to forties.
*	Fashion, Food and Beauty Enthusiasts (Cluster 2): Customer interests include fashion, beauty and cooking.
*	College Students (Cluster 3): This group of customers tweet daily about their college life, online gaming and sports playing.
*	Parents (Cluster 4): Customer interests include parenting, family, religion, school
*	Middle-aged working-class men (Cluster 5): Customer interests include politics, travel, automotive.
*	Fitness Enthusiasts (Cluster 6): Customer interests include health & nutrition, personal fitness, outdoors.

```{r}
#cluster1
qplot(shopping, photo_sharing, data=S, col=factor(k$cluster), xlab='shopping', 
      ylab='photo sharing')

#cluster2
qplot(fashion, beauty, data=S, col=factor(k$cluster), xlab='fashion', 
      ylab='beauty')

#cluster3
qplot(college_uni, online_gaming, data=S, col=factor(k$cluster), xlab='College_uni', 
      ylab='Online Gaming')

#cluster4
qplot(religion, family, data=S, col=factor(k$cluster), xlab='religion', 
      ylab='family')
qplot(sports_fandom, parenting, data=S, col=factor(k$cluster), xlab='sports fandom', 
      ylab='parenting')

#cluster5
qplot(politics, automotive, data=S, col=factor(k$cluster), xlab='politics', 
      ylab='automotive')

#cluster6
qplot(health_nutrition, personal_fitness, data=S, col=factor(k$cluster), xlab='health & nutrition', 
      ylab='personal fitness')
```

# Question 5 - Author Attribution

## Loading necessary libraries

```{r echo = T, results="hide"}
library(tm) 
library(magrittr)
library(slam)
library(proxy)
library('e1071')  # for naive bayes model
library(caret)
```

## Reading Train and Test data from the files

```{r echo = T}
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') 
}
train_list = Sys.glob('C:/Users/Pranay/OneDrive/Documents/STA 380/STA380/data/ReutersC50/C50train/*/*.txt')
train_data = lapply(train_list, readerPlain) 
test_list = Sys.glob('C:/Users/Pranay/OneDrive/Documents/STA 380/STA380/data/ReutersC50/C50test/*/*.txt')
test_data = lapply(test_list, readerPlain) 

#Function to clean the names :

clean_list = function(list1) 
{
   clean_lst<- list1 %>%
    { strsplit(., '/', fixed=TRUE) } %>%
    { lapply(., tail, n=2) } %>%
    { lapply(., paste0, collapse = '') } %>%
    unlist
   
   return(clean_lst)
}

train_names=clean_list(train_list)
names(train_data) = train_names

test_names=clean_list(test_list)
names(test_data) = test_names

# Clean Author name list :

author_train= clean_list(train_list)
author_train= gsub('[0-9]+', '', author_train)
author_train= gsub('newsML.txt', '', author_train)

author_test= clean_list(test_list) 
author_test= gsub('[0-9]+', '', author_test)
author_test= gsub('newsML.txt', '', author_test)

```

## Generating the Corpus for train and test data

```{r echo = T}
documents_raw_train = Corpus(VectorSource(train_data))
documents_raw_test = Corpus(VectorSource(test_data))
```

## Tokenization

```{r echo = T, warning=FALSE}

#Function for text pre-processing 

text_pre_proc= function(dat1) 
{
  my_documents = dat1
  my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase 
  my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
  my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
  my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
  my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))  # remove stop words
  return(my_documents)
}

# Pre-processing for train data :

train_doc=text_pre_proc(documents_raw_train)
test_doc=text_pre_proc(documents_raw_test)

```

## Creating Doc-term-matrix and calculating TF-IDF weights

```{r echo = T, warning=FALSE}

## Function to create doc matrix and fund TF-IDF weights 

DTM_mat = DocumentTermMatrix(train_doc)  # Convert to matrix
DTM_mat = removeSparseTerms(DTM_mat, 0.95) # Removing the longtail terms (5%)


tf_idf_train = weightTfIdf(DTM_mat)

# A suboptimal but practical solution: ignore words you haven't seen before
# can do this by pre-specifying a dictionary in the construction of a DTM
DTM_test = DocumentTermMatrix(test_doc, control=list(dictionary=Terms(DTM_mat)))

tf_idf_test=weightTfIdf(DTM_test)


```

## Dimensionlity Reduction: Principal Component Analysis

```{r echo = T, warning=FALSE}

# Data Pre-Processing for PCA : 
#1. create a matrix 
#2. Remove the columns with 0 values

X_train = as.matrix(tf_idf_train)
X_test = as.matrix(tf_idf_test)

#Removing columns with entries with 0 values
scrub_cols = which(colSums(X_train) == 0)
scrub_cols_test = which(colSums(X_test) == 0)
X_train = X_train[,-scrub_cols]
X_test = X_test[,-scrub_cols_test]

# drop uncommon words
X_test = X_test[,intersect(colnames(X_test),colnames(X_train))]
X_train = X_train[,intersect(colnames(X_test),colnames(X_train))]

```

## Train PCA on training dataset and predict for test dataset

```{r echo = T, warning=FALSE}

pca_train = prcomp(X_train, scale=TRUE)
pca_test=predict(pca_train,newdata = X_test )

```

## Choosing number of PCs to be selected

```{r, results="hide"}
plot(pca_train,type='line')
#summary(pca_train)
vars <- apply(pca_train$x, 2, var)  
props <- vars / sum(vars)
cumsum(props)
```

Choosing 75% varability hence taking 338 PCs (As both train and train1 )

## Create the final dataset with reduced dimensions and with authornames

```{r echo = T,warning=FALSE}
final_train = data.frame(pca_train$x[,1:338])
final_train['author']=author_train
# Form PCs similar to training dataset
loading_train <- pca_train$rotation[,1:338]

# multiply to get a test matrix with the principal component values
X_test_pc <- scale(X_test) %*% loading_train
final_test = data.frame(pca_test[,1:338])
final_test['author']=author_test

```

## LDA

```{r, results="hide", warning=FALSE}
library(MASS)
lda_model <- lda(as.factor(author)~., data=final_train)

lda_pred <- predict(lda_model, newdata = final_test)

#Calculate the accuracy:

answer_lda <- as.data.frame(cbind(lda_pred, final_test$author))
answer_lda$correct <- ifelse(lda_pred$class==final_test$author, 1, 0)
sum(answer_lda$correct )*100/nrow(answer_lda)

```

## Naive Bayes model 

```{r echo = T, warning=FALSE}

naive_model =naiveBayes(as.factor(author) ~., data=final_train)
naive_pred = predict(naive_model,final_test)
# Calculating the accuracy
answer_naive <- as.data.frame(cbind(naive_pred, final_test$author))
answer_naive$correct <- ifelse(naive_pred==final_test$author, 1, 0)
sum(answer_naive$correct)*100/nrow(answer_naive)

```

## Random Forest 

```{r echo = T, results="hide", warning=FALSE}
library('randomForest')
rf_model = randomForest(as.factor(author) ~ ., data=final_train, ntree=1000, importance=TRUE)
rf_pred_test = predict(rf_model,final_test, type='response')
# Calculating the accuracy
answer <- as.data.frame(cbind(rf_pred_test, final_test$author))
answer$correct <- ifelse(rf_pred_test==final_test$author, 1, 0)
sum(answer$correct )*100/nrow(answer)
```

## KNN

```{r, results="hide"}
library(kknn)
accuracies = list()
for (i in c(7,9,11,13,15,17))
{
  knn_model = kknn(as.factor(author) ~ ., final_train, final_test,
                    distance = 1,
                    k= i,
                    kernel = 'rectangular')
  
  accuracies <- c(accuracies,sum(knn_model$fitted.values == final_test$author)/nrow(final_test))
}
plot(c(7,9,11,13,15,17), accuracies, main = "KNN accuracy vs K", xlab = "K-Values", ylab = "Accuracy Score", lty = 1)
```

Considering 10 nearest neighbours 

```{r,echo = FALSE}
# Build knn model with 10 nearest neightbours 

knn_model_vf = kknn(as.factor(author) ~ ., final_train, final_test,
                    distance = 1,
                    k= 10,
                    kernel = 'rectangular')

# calculate the accuracy 
pred_knn <- as.data.frame(cbind(knn_model$fitted.values, final_test$author))
pred_knn$correct <- ifelse(knn_model$fitted.values== final_test$author, 1, 0)

knn_acc= sum(pred_knn$correct)*100/nrow(pred_knn)
knn_acc

```

## Findings:

- LDA gave us the best accuracy (58.92%) among all the models we tried out (Naive Bayes, Random forest and KNN)

## Methodology:
- We removed terms which don't appear in 95% of the documents
- We used Principle component analysis (PCA) to reduce the dimensions as including all the dimensions becomes computationally heavy However, accuracy is compromised due to reducing the number of dimensions 

# Quesion 6 - Association Rules

```{r, results="hide"}
# Association rule mining
rm(list=ls())

library(tidyverse)
library(arules)  
library(arulesViz)

groceries = read.csv('C:/Users/Pranay/OneDrive/Documents/STA 380/STA380/data/groceries.txt', header=F, stringsAsFactors = F)
```

## EDA

```{r}
str(groceries)
# number of baskets
dim(groceries)[1]

# distribution of items sold
product_list = c(as.list(groceries['V1']), as.list(groceries['V2']),as.list(groceries['V3']), as.list(groceries['V4']))

product_list = unlist(product_list)
freq_of_products = as.data.frame(table(product_list))[-1,]
# total number of products sold
sum(freq_of_products['Freq'])
# number of unique products sold
dim(freq_of_products)[1]

# most popular products
data_plot = freq_of_products %>% arrange(-Freq) %>% head(20)

ggplot(data=data_plot,aes(x=reorder(product_list,Freq), y=Freq)) + 
  geom_bar(stat='identity') + 
    labs(title="Most popular products", 
       y="Frequency",
       x = "Products") +
  coord_flip() 

# least popular products
data_plot = freq_of_products %>% arrange(Freq) %>% head(20)

ggplot(data=data_plot,aes(x=reorder(product_list,-Freq), y=Freq)) + 
  geom_bar(stat='identity') + 
    labs(title="Least popular products", 
       y="Frequency",
       x = "Products") +
  coord_flip() 

```

## Findings:
- Most frequently bought item is whole milk
- Number of unique items bought is 169
- Total number of products sold is 43367

## Association rules

```{r}
grocery_baskets = read.transactions(file="C:/Users/Pranay/OneDrive/Documents/STA 380/STA380/data/groceries.txt", rm.duplicates=F, format="basket",sep=",",cols=1,header=F)

grocery_trans = as(grocery_baskets, "transactions")
summary(grocery_trans)

# Whole milk is the most popular product as confirmed in our EDA
groceryrules = apriori(grocery_trans, 
	parameter=list(support=.05, confidence=.1, maxlen=1))
                         
# Look at the output rules
arules::inspect(groceryrules)

groceryrules = apriori(grocery_trans, 
	parameter=list(support=.001, confidence=.50, maxlen=4))
                         
# Look at the output rules
arules::inspect(groceryrules)

plot(head(groceryrules, n = 10, by ="lift"), method = "graph", 
     main = "Top 10 Association Rules")

```

## Findings:
- Whole milk is the most commonly bought item which matches our findings from the EDA
- People frequently add other salty snacks when they buy soda and popcorn
- Liquor, blush wine and bottled beer are sold together
- Baking items - flour, baking powder and sugar are bought together
- Dairy products - Hard cheese, butter, whipped/sour cream and yogurt are bought together
- White bread, domestic eggs and processed cheese sell together etc.
